{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b78e6999",
   "metadata": {},
   "source": [
    "# Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9ad29a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_matched = pd.read_excel(\"del7_ADRD_with_map_3new.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "812dc07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study_id', 'study_enc_id', 'study_case_id', 'duration', 'del_1', 'year-proc', 'sex', 'race', 'ethnicity', 'age', 'asa_class', 'asa1', 'asa1e', 'asa2', 'asa2e', 'asa3', 'asa3e', 'asa4', 'asa4e', 'asa5', 'asa5e', 'asa6', 'asa6e', 'CCI', 'cisatracurium', 'hydromorphone', 'lorazepam', 'midazolam', 'sufentanil', 'alfentanil', 'dexmedetomidine', 'etomidate', 'fentanyl', 'ketamine', 'methadone', 'meperidine', 'morphine', 'propofol', 'remifentanil', 'rocuronium', 'succinylcholine', 'vecuronium', 'diazepam', 'map_min', 'map_max', 'esmolol', 'hydralazine', 'labetalol', 'metoprolol', 'nicardipine', 'nitroprusside', 'enalapril', 'epinephrine', 'norepinephrine', 'phenylephrine', 'ephedrine', 'vasopressin', 'dopamine', 'milrinone', 'dobutamine', 'SE', 'Anemia', 'CKD', 'COPD', 'Cancer', 'Cancer METS', 'Cardiac arrhythmia', 'Cerebral Vascular Dz', 'Chr Dialysis', 'Chr ETOH', 'Chr heart failure', 'Diabetes', 'Disease of the AORTA', 'Drug Abuse', 'HTN', 'Heart valve dz', 'Hemiplegia/Paraplegia', 'Ischemic heart dz', 'LIVER DZ', 'Obesity', 'Peripheral vascular disease', 'Psychiatric Disorders', 'Pulmonaru Vasular Dz', 'Resp Failure', 'Transplanted Organ', 'anti histaminics', 'Antispasmotics', 'sedatives', 'antihypertensives', 'Gastrointestinal Agents', 'opioids and NSAIDS', 'psychoactive and anticonvulsant ', 'steroids', 'antipsychotics', 'anti-arrhythmics', 'antibiotics', 'total_mac_hrs', 'inhal_anesthesia', 'General Surgery', 'Vascular', 'Orthopedics', 'Thoracic', 'Neurosurgery', 'GYN', 'GU', 'ENT', 'EP/PACEMAKER', 'GI-NORA', 'EYE', 'Dementia', 'map_lt_65_min', 'map_65_95_min', 'map_gt_95_min']\n"
     ]
    }
   ],
   "source": [
    "print(df_dropped.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c513aeb1",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0921bb95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>case_x</th>\n",
       "      <th>study_id</th>\n",
       "      <th>study_enc_id</th>\n",
       "      <th>study_case_id</th>\n",
       "      <th>duration</th>\n",
       "      <th>outcome</th>\n",
       "      <th>del_1</th>\n",
       "      <th>del_8</th>\n",
       "      <th>year-proc</th>\n",
       "      <th>...</th>\n",
       "      <th>GU</th>\n",
       "      <th>ENT</th>\n",
       "      <th>EP/PACEMAKER</th>\n",
       "      <th>GI-NORA</th>\n",
       "      <th>EYE</th>\n",
       "      <th>case_y</th>\n",
       "      <th>Dementia</th>\n",
       "      <th>map_lt_65_min</th>\n",
       "      <th>map_65_95_min</th>\n",
       "      <th>map_gt_95_min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 126 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Unnamed: 0, case_x, study_id, study_enc_id, study_case_id, duration, outcome, del_1, del_8, year-proc, sex, race, ethnicity, age, asa_class, age.1, asa_class.1, asa1, asa1e, asa2, asa2e, asa3, asa3e, asa4, asa4e, asa5, asa5e, asa6, asa6e, CCI, cisatracurium, hydromorphone, lorazepam, midazolam, sufentanil, alfentanil, dexmedetomidine, etomidate, fentanyl, ketamine, methadone, meperidine, morphine, propofol, remifentanil, rocuronium, succinylcholine, vecuronium, diazepam, map_mean, map_min, map_max, esmolol, hydralazine, labetalol, metoprolol, nicardipine, nitroprusside, enalapril, epinephrine, norepinephrine, phenylephrine, ephedrine, vasopressin, dopamine, milrinone, dobutamine, SE, Anemia, CKD, COPD, Cancer, Cancer METS, Cardiac arrhythmia, Cerebral Vascular Dz, Chr Dialysis, Chr ETOH, Chr heart failure, Diabetes, Disease of the AORTA, Drug Abuse, HTN, Heart valve dz, Hemiplegia/Paraplegia, Ischemic heart dz, LIVER DZ, Obesity, Peripheral vascular disease, Psychiatric Disorders, Pulmonaru Vasular Dz, Resp Failure, Transplanted Organ, anti histaminics, Antimuscarinics/Incontinence Meds, Antispasmotics, sedatives, antihypertensives, Gastrointestinal Agents, Respiratory Medications, opioids and NSAIDS, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 126 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matched[df_matched[\"outcome\"] != df_matched[\"del_1\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f19c3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suspects unique values:\n",
      "anti-parkinsonian: [0]\n",
      "antidepressants: [0]\n",
      "Dementia: [0 1]\n",
      "Muscle Relaxants: [0 1]\n",
      "Antimuscarinics/Incontinence Meds: [0]\n"
     ]
    }
   ],
   "source": [
    "suspects = ['anti-parkinsonian', 'antidepressants', 'Dementia', 'Muscle Relaxants', 'Antimuscarinics/Incontinence Meds']\n",
    "print(\"suspects unique values:\")\n",
    "for col in suspects:\n",
    "    print(f\"{col}: {df_matched[col].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12593939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ethnicity\n",
       "Not Hispanic or Latino    5562\n",
       "Hispanic or Latino          98\n",
       "Unknown                     52\n",
       "Declined                     6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matched['ethnicity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa5c457",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e71b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dropped = df_matched.drop(columns=['map_mean', 'outcome','Case','Muscle Relaxants', 'case_x', 'case_y', 'del_8', 'Unnamed: 0', 'case', 'Antimuscarinics/Incontinence Meds', 'asa_class.1', 'age.1', 'anti-parkinsonian', 'antidepressants', 'Respiratory Medications', 'study_id_case', 'study_enc_id_case', 'study_case_id_case'], errors=\"ignore\")\n",
    "df_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57950d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719061cd",
   "metadata": {},
   "source": [
    "## Medication Class Binarization (Presence/Absence Encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63416ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_cols = [\n",
    "    'anti histaminics', 'Antispasmotics', 'sedatives',\n",
    "    'antihypertensives', 'Gastrointestinal Agents',\n",
    "    'opioids and NSAIDS', 'psychoactive and anticonvulsant ',\n",
    "    'steroids', 'anti-arrhythmics', 'antibiotics'\n",
    "]\n",
    "\n",
    "# Keep originals with suffix \"_orig\"\n",
    "for col in mask_cols:\n",
    "    df_dropped[col + \"_orig\"] = df_dropped[col]\n",
    "    df_dropped[col] = (df_dropped[col] > 0).astype(int)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6274545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names (strip, lower, replace spaces)\n",
    "df_dropped.columns = (\n",
    "    df_dropped.columns\n",
    "      .str.strip()\n",
    "      .str.replace(r'\\s+', ' ', regex=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6162ddd4",
   "metadata": {},
   "source": [
    "## Feature Engineering for ASA Class and Emergency Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "301bf4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped _orig columns. Remaining columns: 113\n",
      "Created asa_class_num + asa_emerg_flag and dropped 12 ASA dummies.\n",
      "  asa_class  asa_class_num  asa_emerg_flag\n",
      "0         3            3.0               0\n",
      "1         3            3.0               0\n",
      "2         3            3.0               0\n",
      "3         3            3.0               0\n",
      "4         4            4.0               0\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Drop \"_orig\" columns ---\n",
    "df_mod = df_dropped.loc[:, ~df_dropped.columns.str.endswith(\"_orig\")].copy()\n",
    "print(f\"Dropped _orig columns. Remaining columns: {df_mod.shape[1]}\")\n",
    "\n",
    "# --- Step 2: Create ASA numeric and emergency flag ---\n",
    "# ASA class numeric: extract from asa_class (if present)\n",
    "if \"asa_class\" in df_mod.columns:\n",
    "    # Coerce text to numeric safely\n",
    "    df_mod[\"asa_class_num\"] = (\n",
    "        df_mod[\"asa_class\"]\n",
    "        .astype(str)\n",
    "        .str.extract(r\"(\\d)\")\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# Emergency flag: 1 if any of the ASA_emergency variants (asa1e–asa6e) are 1\n",
    "asa_emerg_cols = [c for c in df_mod.columns if c.lower().startswith(\"asa\") and c.lower().endswith(\"e\")]\n",
    "df_mod[\"asa_emerg_flag\"] = df_mod[asa_emerg_cols].max(axis=1) if asa_emerg_cols else 0\n",
    "\n",
    "# --- Step 3: Drop individual ASA one-hot columns (asa1–asa6 and asa1e–asa6e) ---\n",
    "asa_cols_to_drop = [c for c in df_mod.columns if c.lower().startswith(\"asa\") and (c.lower().endswith(tuple(str(i) for i in range(1, 7))) or c.lower().endswith(tuple(f\"{i}e\" for i in range(1, 7))))]\n",
    "df_mod.drop(columns=asa_cols_to_drop, inplace=True, errors=\"ignore\")\n",
    "\n",
    "print(f\"Created asa_class_num + asa_emerg_flag and dropped {len(asa_cols_to_drop)} ASA dummies.\")\n",
    "print(df_mod[[\"asa_class\", \"asa_class_num\", \"asa_emerg_flag\"]].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1818387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   asa_class  asa_class_num  asa_emerg_flag\n",
      "0          3            3.0               0\n",
      "1          3            3.0               0\n",
      "2          3            3.0               0\n",
      "3          3            3.0               0\n",
      "4          4            4.0               0\n",
      "5          3            3.0               0\n",
      "6          4            4.0               0\n",
      "7          4            4.0               0\n",
      "8         3E            3.0               1\n",
      "9         4E            4.0               1\n",
      "10         3            3.0               0\n",
      "11        4E            4.0               1\n",
      "12        4E            4.0               1\n",
      "13        5E            5.0               1\n",
      "14        5E            5.0               1\n",
      "15        4E            4.0               1\n",
      "16         3            3.0               0\n",
      "17        4E            4.0               1\n",
      "18         3            3.0               0\n",
      "19         3            3.0               0\n"
     ]
    }
   ],
   "source": [
    "print(df_mod[[\"asa_class\", \"asa_class_num\", \"asa_emerg_flag\"]].head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c09e54",
   "metadata": {},
   "source": [
    "## Phik-Based Feature Correlation Screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7278b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly correlated pairs (>|0.8|):\n",
      "         col1                            col2     phik\n",
      " study_enc_id                   study_case_id 0.999921\n",
      "     ketamine psychoactive and anticonvulsant 0.998867\n",
      "    midazolam                       sedatives 0.996790\n",
      "study_case_id                       year-proc 0.990390\n",
      " study_enc_id                       year-proc 0.990062\n",
      "     fentanyl              opioids and NSAIDS 0.975181\n",
      "          CKD                    Chr Dialysis 0.971636\n",
      "   metoprolol               antihypertensives 0.948791\n",
      "     Chr ETOH                      Drug Abuse 0.839514\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Drop redundant column ---\n",
    "df_phik = df_mod.drop(columns=[\"asa_class\"], errors=\"ignore\").copy()\n",
    "\n",
    "# --- 2. Import and compute Phik correlation matrix ---\n",
    "from phik import phik_matrix\n",
    "from phik.report import plot_correlation_matrix\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "# Numeric columns (for better binning)\n",
    "interval_cols = df_phik.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Compute Phik correlation matrix\n",
    "phik_mat = phik_matrix(df_phik, interval_cols=interval_cols)\n",
    "\n",
    "# --- 3. Flatten into long-form pairs ---\n",
    "np.fill_diagonal(phik_mat.values, np.nan)\n",
    "mask_upper = np.triu(np.ones_like(phik_mat, dtype=bool), k=1)\n",
    "pairs = (\n",
    "    phik_mat.where(mask_upper)\n",
    "            .stack()\n",
    "            .reset_index()\n",
    "            .rename(columns={\"level_0\": \"col1\", \"level_1\": \"col2\", 0: \"phik\"})\n",
    "            .sort_values(\"phik\", ascending=False)\n",
    ")\n",
    "\n",
    "# --- 4. View highly correlated pairs (>|0.8|) ---\n",
    "high_pairs = pairs.loc[pairs[\"phik\"] > 0.8].reset_index(drop=True)\n",
    "print(\"Highly correlated pairs (>|0.8|):\")\n",
    "print(high_pairs.head(50).to_string(index=False))\n",
    "\n",
    "# Optional visualization (good for overview)\n",
    "# plot_correlation_matrix(phik_mat.values, x_labels=phik_mat.columns, y_labels=phik_mat.index, vmin=0.5, vmax=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13cd66bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 6 highly correlated columns:\n",
      "['psychoactive and anticonvulsant', 'sedatives', 'Chr Dialysis', 'opioids and NSAIDS', 'antihypertensives', 'Drug Abuse']\n",
      "\n",
      "Remaining columns: 97\n"
     ]
    }
   ],
   "source": [
    "# --- Drop redundant high-correlation features (based on expert review) ---\n",
    "drop_cols = [\n",
    "    \"psychoactive and anticonvulsant\",\n",
    "    \"sedatives\",\n",
    "    \"Chr Dialysis\",\n",
    "    \"opioids and NSAIDS\",\n",
    "    \"antihypertensives\",\n",
    "    \"Drug Abuse\"\n",
    "]\n",
    "\n",
    "df_mod = df_mod.drop(columns=drop_cols, errors=\"ignore\")\n",
    "\n",
    "print(f\"Dropped {len(drop_cols)} highly correlated columns:\")\n",
    "print(drop_cols)\n",
    "print(f\"\\nRemaining columns: {df_mod.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3726f76f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Not Hispanic or Latino' 'Unknown' nan 'Hispanic or Latino' 'Declined']\n"
     ]
    }
   ],
   "source": [
    "unique_names = df_mod['ethnicity'].unique()\n",
    "print(unique_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46df6350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['study_id', 'study_enc_id', 'study_case_id', 'duration', 'del_1', 'year-proc', 'sex', 'race', 'ethnicity', 'age', 'asa_class', 'CCI', 'cisatracurium', 'hydromorphone', 'lorazepam', 'midazolam', 'sufentanil', 'alfentanil', 'dexmedetomidine', 'etomidate', 'fentanyl', 'ketamine', 'methadone', 'meperidine', 'morphine', 'propofol', 'remifentanil', 'rocuronium', 'succinylcholine', 'vecuronium', 'diazepam', 'map_min', 'map_max', 'esmolol', 'hydralazine', 'labetalol', 'metoprolol', 'nicardipine', 'nitroprusside', 'enalapril', 'epinephrine', 'norepinephrine', 'phenylephrine', 'ephedrine', 'vasopressin', 'dopamine', 'milrinone', 'dobutamine', 'SE', 'Anemia', 'CKD', 'COPD', 'Cancer', 'Cancer METS', 'Cardiac arrhythmia', 'Cerebral Vascular Dz', 'Chr ETOH', 'Chr heart failure', 'Diabetes', 'Disease of the AORTA', 'HTN', 'Heart valve dz', 'Hemiplegia/Paraplegia', 'Ischemic heart dz', 'LIVER DZ', 'Obesity', 'Peripheral vascular disease', 'Psychiatric Disorders', 'Pulmonaru Vasular Dz', 'Resp Failure', 'Transplanted Organ', 'anti histaminics', 'Antispasmotics', 'Gastrointestinal Agents', 'steroids', 'antipsychotics', 'anti-arrhythmics', 'antibiotics', 'total_mac_hrs', 'inhal_anesthesia', 'General Surgery', 'Vascular', 'Orthopedics', 'Thoracic', 'Neurosurgery', 'GYN', 'GU', 'ENT', 'EP/PACEMAKER', 'GI-NORA', 'EYE', 'Dementia', 'map_lt_65_min', 'map_65_95_min', 'map_gt_95_min', 'asa_class_num', 'asa_emerg_flag']\n"
     ]
    }
   ],
   "source": [
    "print(df_mod.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9cf090",
   "metadata": {},
   "source": [
    "## Missing values check and imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4424a93b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Missing value summary (only columns with >0 missing) ===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_missing</th>\n",
       "      <th>pct_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <td>30</td>\n",
       "      <td>0.523652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asa_class</th>\n",
       "      <td>1</td>\n",
       "      <td>0.017455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>asa_class_num</th>\n",
       "      <td>1</td>\n",
       "      <td>0.017455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               n_missing  pct_missing\n",
       "ethnicity             30     0.523652\n",
       "asa_class              1     0.017455\n",
       "asa_class_num          1     0.017455"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total columns with missing values: 3\n",
      "Total rows: 5729\n"
     ]
    }
   ],
   "source": [
    "# --- Missing value summary for df_dropped ---\n",
    "import pandas as pd\n",
    "\n",
    "missing_summary = (\n",
    "    df_mod.isna()\n",
    "    .sum()\n",
    "    .to_frame(\"n_missing\")\n",
    "    .assign(pct_missing=lambda d: 100 * d[\"n_missing\"] / len(df_mod))\n",
    "    .query(\"n_missing > 0\")  # only columns with missing values\n",
    "    .sort_values(\"pct_missing\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"=== Missing value summary (only columns with >0 missing) ===\")\n",
    "display(missing_summary.head(30))  # show top 30 for quick scan\n",
    "print(f\"\\nTotal columns with missing values: {missing_summary.shape[0]}\")\n",
    "print(f\"Total rows: {len(df_mod)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "761cfc05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining missing values after imputation:\n",
      "Series([], dtype: int64)\n"
     ]
    }
   ],
   "source": [
    "# --- Handle missing values in df_mod ---\n",
    "df_mod = df_mod.copy()\n",
    "\n",
    "# 1) Fill missing ethnicity with 'Unknown'\n",
    "df_mod[\"ethnicity\"] = df_mod[\"ethnicity\"].fillna(\"Unknown\")\n",
    "\n",
    "# 2) Impute ASA class and ASA class_num\n",
    "if \"asa_class\" in df_mod.columns:\n",
    "    mode_asa = df_mod[\"asa_class\"].mode(dropna=True)[0]\n",
    "    df_mod[\"asa_class\"] = df_mod[\"asa_class\"].fillna(mode_asa)\n",
    "\n",
    "if \"asa_class_num\" in df_mod.columns:\n",
    "    median_asa_num = df_mod[\"asa_class_num\"].median()\n",
    "    df_mod[\"asa_class_num\"] = df_mod[\"asa_class_num\"].fillna(median_asa_num)\n",
    "\n",
    "# --- Quick verification ---\n",
    "print(\"Remaining missing values after imputation:\")\n",
    "print(df_mod.isna().sum().loc[lambda x: x > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2d052",
   "metadata": {},
   "source": [
    "# Feature Schema Definition for Reproducible Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "15a6f968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → artifacts/feature_schema.json\n",
      "Counts → continuous=10, categorical=3, binary=78, constants=0\n",
      "Categoricals: ['ethnicity', 'race', 'sex']\n",
      "Target prevalence: 0.500\n",
      "Excluded (not modeled): ['del_1', 'study_id', 'study_enc_id', 'study_case_id', 'year-proc', 'asa_class']\n"
     ]
    }
   ],
   "source": [
    "# --- Build & save feature schema from df_mod (final) ---\n",
    "import json, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "TARGET = \"del_1\"\n",
    "\n",
    "# Exclude IDs & admin columns from features, but keep them in df_mod for CV grouping\n",
    "ID_COLS = [\"study_id\", \"study_enc_id\", \"study_case_id\"]\n",
    "ADMIN_COLS = [\"year-proc\", \"asa_class\"]  # asa_class_num stays as continuous; raw asa_class is redundant\n",
    "EXCLUDE = [TARGET] + ID_COLS + ADMIN_COLS\n",
    "\n",
    "df = df_mod.copy()\n",
    "\n",
    "# 1) Constants (<=1 unique non-null)\n",
    "constant_cols = (\n",
    "    df.drop(columns=EXCLUDE, errors=\"ignore\")\n",
    "      .nunique(dropna=True)\n",
    "      .pipe(lambda s: s[s <= 1].index.tolist())\n",
    ")\n",
    "\n",
    "# 2) Split by dtype\n",
    "num_like = df.select_dtypes(include=[np.number, \"boolean\", \"bool\"]) \\\n",
    "             .drop(columns=EXCLUDE + constant_cols, errors=\"ignore\")\n",
    "obj_like = df.select_dtypes(include=[\"object\", \"category\"]) \\\n",
    "             .drop(columns=EXCLUDE + constant_cols, errors=\"ignore\")\n",
    "\n",
    "# 3) Detect strict 0/1 binaries among numerics\n",
    "def is_binary(series: pd.Series) -> bool:\n",
    "    vals = pd.Series(series).dropna().unique()\n",
    "    return len(vals) > 0 and set(vals).issubset({0, 1})\n",
    "\n",
    "binary_cols = [c for c in num_like.columns if is_binary(num_like[c])]\n",
    "continuous_cols = sorted(set(num_like.columns) - set(binary_cols))\n",
    "categorical_cols = sorted(obj_like.columns.tolist())\n",
    "\n",
    "# 4) Enforce ASA design explicitly (keep numeric; flag emergency as binary)\n",
    "if \"asa_class_num\" in df.columns:\n",
    "    if \"asa_class_num\" in binary_cols:   binary_cols.remove(\"asa_class_num\")\n",
    "    if \"asa_class_num\" not in continuous_cols and \"asa_class_num\" not in constant_cols:\n",
    "        continuous_cols.append(\"asa_class_num\")\n",
    "\n",
    "if \"asa_emerg_flag\" in df.columns and is_binary(df[\"asa_emerg_flag\"]):\n",
    "    if \"asa_emerg_flag\" in continuous_cols: continuous_cols.remove(\"asa_emerg_flag\")\n",
    "    if \"asa_emerg_flag\" not in binary_cols and \"asa_emerg_flag\" not in constant_cols:\n",
    "        binary_cols.append(\"asa_emerg_flag\")\n",
    "\n",
    "# 5) Final feature list (exclude target, IDs, admin, constants)\n",
    "feature_list = [c for c in df.columns if c not in set(EXCLUDE + constant_cols)]\n",
    "\n",
    "# Ensure mutual exclusivity across buckets (priority: categorical > binary > continuous)\n",
    "seen = set(); final_cats, final_bins, final_conts = [], [], []\n",
    "for c in sorted(categorical_cols):\n",
    "    if c in feature_list and c not in seen: final_cats.append(c); seen.add(c)\n",
    "for c in sorted(binary_cols):\n",
    "    if c in feature_list and c not in seen: final_bins.append(c); seen.add(c)\n",
    "for c in sorted(continuous_cols):\n",
    "    if c in feature_list and c not in seen: final_conts.append(c); seen.add(c)\n",
    "\n",
    "# Ordered feature list (optional; your choice)\n",
    "feature_list = final_conts + final_cats + final_bins\n",
    "\n",
    "# 6) Label stats\n",
    "label_stats = {TARGET: float(pd.Series(df[TARGET]).mean())}\n",
    "\n",
    "# 7) Save schema\n",
    "schema = {\n",
    "    \"target\": TARGET,\n",
    "    \"aux_outcomes\": [],\n",
    "    \"binary_cols\": sorted(final_bins),\n",
    "    \"continuous_cols\": sorted(final_conts),\n",
    "    \"categorical_cols\": sorted(final_cats),\n",
    "    \"constant_cols\": sorted(constant_cols),\n",
    "    \"feature_list\": feature_list,\n",
    "    \"dtypes\": {c: str(df[c].dtype) for c in feature_list},\n",
    "    \"label_stats\": label_stats,\n",
    "    \"exclude_cols\": EXCLUDE,        # helpful breadcrumb\n",
    "    \"id_cols\": ID_COLS,             # for grouping later\n",
    "}\n",
    "\n",
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "with open(\"artifacts/feature_schema.json\", \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "print(\"Saved → artifacts/feature_schema.json\")\n",
    "print(f\"Counts → continuous={len(schema['continuous_cols'])}, categorical={len(schema['categorical_cols'])}, binary={len(schema['binary_cols'])}, constants={len(schema['constant_cols'])}\")\n",
    "print(\"Categoricals:\", schema[\"categorical_cols\"])\n",
    "print(\"Target prevalence:\", f\"{schema['label_stats'][TARGET]:.3f}\")\n",
    "print(\"Excluded (not modeled):\", EXCLUDE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc4fc5af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CCI',\n",
       " 'age',\n",
       " 'asa_class_num',\n",
       " 'duration',\n",
       " 'map_65_95_min',\n",
       " 'map_gt_95_min',\n",
       " 'map_lt_65_min',\n",
       " 'map_max',\n",
       " 'map_min',\n",
       " 'total_mac_hrs']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "continuous_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566e1c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cisatracurium',\n",
       " 'hydromorphone',\n",
       " 'lorazepam',\n",
       " 'midazolam',\n",
       " 'sufentanil',\n",
       " 'alfentanil',\n",
       " 'dexmedetomidine',\n",
       " 'etomidate',\n",
       " 'fentanyl',\n",
       " 'ketamine',\n",
       " 'methadone',\n",
       " 'meperidine',\n",
       " 'morphine',\n",
       " 'propofol',\n",
       " 'remifentanil',\n",
       " 'rocuronium',\n",
       " 'succinylcholine',\n",
       " 'vecuronium',\n",
       " 'diazepam',\n",
       " 'esmolol',\n",
       " 'hydralazine',\n",
       " 'labetalol',\n",
       " 'metoprolol',\n",
       " 'nicardipine',\n",
       " 'nitroprusside',\n",
       " 'enalapril',\n",
       " 'epinephrine',\n",
       " 'norepinephrine',\n",
       " 'phenylephrine',\n",
       " 'ephedrine',\n",
       " 'vasopressin',\n",
       " 'dopamine',\n",
       " 'milrinone',\n",
       " 'dobutamine',\n",
       " 'SE',\n",
       " 'Anemia',\n",
       " 'CKD',\n",
       " 'COPD',\n",
       " 'Cancer',\n",
       " 'Cancer METS',\n",
       " 'Cardiac arrhythmia',\n",
       " 'Cerebral Vascular Dz',\n",
       " 'Chr ETOH',\n",
       " 'Chr heart failure',\n",
       " 'Diabetes',\n",
       " 'Disease of the AORTA',\n",
       " 'HTN',\n",
       " 'Heart valve dz',\n",
       " 'Hemiplegia/Paraplegia',\n",
       " 'Ischemic heart dz',\n",
       " 'LIVER DZ',\n",
       " 'Obesity',\n",
       " 'Peripheral vascular disease',\n",
       " 'Psychiatric Disorders',\n",
       " 'Pulmonaru Vasular Dz',\n",
       " 'Resp Failure',\n",
       " 'Transplanted Organ',\n",
       " 'anti histaminics',\n",
       " 'Antispasmotics',\n",
       " 'Gastrointestinal Agents',\n",
       " 'steroids',\n",
       " 'antipsychotics',\n",
       " 'anti-arrhythmics',\n",
       " 'antibiotics',\n",
       " 'inhal_anesthesia',\n",
       " 'General Surgery',\n",
       " 'Vascular',\n",
       " 'Orthopedics',\n",
       " 'Thoracic',\n",
       " 'Neurosurgery',\n",
       " 'GYN',\n",
       " 'GU',\n",
       " 'ENT',\n",
       " 'EP/PACEMAKER',\n",
       " 'GI-NORA',\n",
       " 'EYE',\n",
       " 'Dementia',\n",
       " 'asa_emerg_flag']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f16339b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ethnicity', 'race', 'sex']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575831fb",
   "metadata": {},
   "source": [
    "# Unfitted Feature Preprocessing for Cross-Validated Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "756b57fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved → artifacts/preprocessor_unfitted.joblib\n",
      "Spec   → artifacts/preprocessor_spec.json\n",
      "Blocks → continuous=10, categorical=3, binary=78\n",
      "Not modeled (kept only for grouping/metadata): ['del_1', 'study_id', 'study_enc_id', 'study_case_id', 'year-proc', 'asa_class']\n"
     ]
    }
   ],
   "source": [
    "# A) Build & save UNFITTED preprocessor (no leakage, no model)\n",
    "import json\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 0) Load schema produced earlier\n",
    "with open(\"artifacts/feature_schema.json\", \"r\") as f:\n",
    "    schema = json.load(f)\n",
    "\n",
    "TARGET            = schema[\"target\"]\n",
    "continuous_cols   = schema[\"continuous_cols\"]\n",
    "categorical_cols  = schema[\"categorical_cols\"]   # <-- fixed spelling\n",
    "binary_cols       = schema[\"binary_cols\"]\n",
    "EXCLUDED          = schema.get(\"exclude_cols\", [])\n",
    "ID_COLS           = schema.get(\"id_cols\", [\"study_id\", \"study_enc_id\", \"study_case_id\"])\n",
    "\n",
    "# 1) Resolve columns actually present in df_mod (guard against drift)\n",
    "present_cont = [c for c in continuous_cols  if c in df_mod.columns]\n",
    "present_cat  = [c for c in categorical_cols if c in df_mod.columns]\n",
    "present_bin  = [c for c in binary_cols      if c in df_mod.columns]\n",
    "\n",
    "# Safety: ensure excluded + target are not in any present_* lists\n",
    "ban = set(EXCLUDED + [TARGET])\n",
    "present_cont = [c for c in present_cont if c not in ban]\n",
    "present_cat  = [c for c in present_cat  if c not in ban]\n",
    "present_bin  = [c for c in present_bin  if c not in ban]\n",
    "\n",
    "# 2) Define unfitted transformers\n",
    "try:\n",
    "    # Newer sklearn (>=1.2)\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)\n",
    "except TypeError:\n",
    "    # Older sklearn\n",
    "    ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse=True)\n",
    "\n",
    "cont_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scale\",  RobustScaler()),\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"ohe\",    ohe),\n",
    "])\n",
    "\n",
    "bin_pipe = Pipeline([\n",
    "    (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "])\n",
    "\n",
    "# 3) ColumnTransformer (UNFITTED) — no fit/transform here\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cont\", cont_pipe, present_cont),\n",
    "        (\"cat\",  cat_pipe,  present_cat),\n",
    "        (\"bin\",  bin_pipe,  present_bin),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    "    verbose_feature_names_out=False\n",
    ")\n",
    "\n",
    "# 4) Save the unfitted preprocessor and a spec for reproducibility\n",
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "joblib.dump(preprocessor, \"artifacts/preprocessor_unfitted.joblib\")\n",
    "\n",
    "spec = {\n",
    "    \"target\": TARGET,\n",
    "    \"excluded\": EXCLUDED,\n",
    "    \"id_cols\": ID_COLS,\n",
    "    \"present_cont\": present_cont,\n",
    "    \"present_cat\": present_cat,\n",
    "    \"present_bin\": present_bin,\n",
    "}\n",
    "with open(\"artifacts/preprocessor_spec.json\", \"w\") as f:\n",
    "    json.dump(spec, f, indent=2)\n",
    "\n",
    "print(\"Saved → artifacts/preprocessor_unfitted.joblib\")\n",
    "print(\"Spec   → artifacts/preprocessor_spec.json\")\n",
    "print(f\"Blocks → continuous={len(present_cont)}, categorical={len(present_cat)}, binary={len(present_bin)}\")\n",
    "print(\"Not modeled (kept only for grouping/metadata):\", EXCLUDED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21a5da2",
   "metadata": {},
   "source": [
    "# Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae66cf2b",
   "metadata": {},
   "source": [
    "## Baseline Logistic Regression with StratifiedGroupKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e91f9541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] ROC-AUC=0.831  PR-AUC=0.807  Brier=0.168\n",
      "[Fold 2] ROC-AUC=0.818  PR-AUC=0.811  Brier=0.175\n",
      "[Fold 3] ROC-AUC=0.803  PR-AUC=0.790  Brier=0.181\n",
      "[Fold 4] ROC-AUC=0.831  PR-AUC=0.831  Brier=0.168\n",
      "[Fold 5] ROC-AUC=0.798  PR-AUC=0.778  Brier=0.184\n",
      "\n",
      "=== Cross-validated (OOF) performance ===\n",
      "ROC-AUC=0.816  PR-AUC=0.802  Brier=0.175\n",
      "\n",
      "Per-fold means ± SD:\n",
      "AUC  : 0.816 ± 0.014\n",
      "PR   : 0.803 ± 0.018\n",
      "Brier: 0.175 ± 0.007\n"
     ]
    }
   ],
   "source": [
    "# B) Baseline Logistic Regression with StratifiedGroupKFold (group = STUDY_ID)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, StratifiedGroupKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "# 0) Load artifacts from Step A\n",
    "preprocessor = joblib.load(\"artifacts/preprocessor_unfitted.joblib\")\n",
    "with open(\"artifacts/preprocessor_spec.json\", \"r\") as f:\n",
    "    spec = json.load(f)\n",
    "\n",
    "TARGET   = spec[\"target\"]\n",
    "EXCLUDED = spec[\"excluded\"]\n",
    "\n",
    "# 1) Build X/y (exclude target + excluded cols); define patient groups\n",
    "drop_cols = list(set(EXCLUDED + [TARGET]))\n",
    "X_all = df_mod.drop(columns=drop_cols).copy()\n",
    "y_all = df_mod[TARGET].astype(int).copy()\n",
    "\n",
    "if \"study_id\" not in df_mod.columns:\n",
    "    raise ValueError(\"study_id column is required for patient-level grouped CV.\")\n",
    "groups = df_mod[\"study_id\"].values\n",
    "\n",
    "# 2) Baseline Logistic Regression (L2)\n",
    "lr = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    solver=\"liblinear\",   # robust with sparse OHE\n",
    "    C=1.0,\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"prep\", preprocessor),  # unfitted; fits within each fold\n",
    "    (\"clf\",  lr),\n",
    "])\n",
    "\n",
    "# 3) StratifiedGroupKFold by patient\n",
    "n_splits = 5\n",
    "cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "split_iter = cv.split(X_all, y_all, groups=groups)\n",
    "\n",
    "# 4) Train/evaluate per fold; collect OOF predictions\n",
    "oof_pred = np.zeros(len(y_all), dtype=float)\n",
    "fold_metrics = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(split_iter, start=1):\n",
    "    X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "    model = pipe_lr\n",
    "    model.fit(X_tr, y_tr)\n",
    "\n",
    "    p = model.predict_proba(X_va)[:, 1]\n",
    "    oof_pred[va_idx] = p\n",
    "\n",
    "    auc  = roc_auc_score(y_va, p)\n",
    "    pr   = average_precision_score(y_va, p)\n",
    "    bri  = brier_score_loss(y_va, p)\n",
    "    fold_metrics.append((auc, pr, bri))\n",
    "    print(f\"[Fold {fold}] ROC-AUC={auc:.3f}  PR-AUC={pr:.3f}  Brier={bri:.3f}\")\n",
    "\n",
    "# 5) Overall OOF performance (primary CV estimate)\n",
    "auc_oof = roc_auc_score(y_all, oof_pred)\n",
    "pr_oof  = average_precision_score(y_all, oof_pred)\n",
    "bri_oof = brier_score_loss(y_all, oof_pred)\n",
    "\n",
    "print(\"\\n=== Cross-validated (OOF) performance ===\")\n",
    "print(f\"ROC-AUC={auc_oof:.3f}  PR-AUC={pr_oof:.3f}  Brier={bri_oof:.3f}\")\n",
    "\n",
    "# 6) (Optional) Per-fold mean ± SD\n",
    "fold_metrics = np.array(fold_metrics)\n",
    "print(\"\\nPer-fold means ± SD:\")\n",
    "print(f\"AUC  : {fold_metrics[:,0].mean():.3f} ± {fold_metrics[:,0].std():.3f}\")\n",
    "print(f\"PR   : {fold_metrics[:,1].mean():.3f} ± {fold_metrics[:,1].std():.3f}\")\n",
    "print(f\"Brier: {fold_metrics[:,2].mean():.3f} ± {fold_metrics[:,2].std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13171a21",
   "metadata": {},
   "source": [
    "# Baseline Suite: LR-ElasticNet, RandomForest, HistGradientBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f44e8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-shikshuk/.local/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:19: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== lr_elasticnet ===\n",
      "[Fold 1] ROC-AUC=0.824  PR-AUC=0.802  Brier=0.173\n",
      "[Fold 2] ROC-AUC=0.806  PR-AUC=0.791  Brier=0.181\n",
      "[Fold 3] ROC-AUC=0.806  PR-AUC=0.784  Brier=0.181\n",
      "[Fold 4] ROC-AUC=0.820  PR-AUC=0.822  Brier=0.176\n",
      "[Fold 5] ROC-AUC=0.798  PR-AUC=0.777  Brier=0.184\n",
      "\n",
      "OOF:\n",
      "ROC-AUC=0.809  PR-AUC=0.792  Brier=0.179\n",
      "Per-fold means ± SD:\n",
      "AUC  : 0.811 ± 0.010\n",
      "PR   : 0.795 ± 0.016\n",
      "Brier: 0.179 ± 0.004\n",
      "\n",
      "=== random_forest ===\n",
      "[Fold 1] ROC-AUC=0.832  PR-AUC=0.810  Brier=0.171\n",
      "[Fold 2] ROC-AUC=0.821  PR-AUC=0.815  Brier=0.176\n",
      "[Fold 3] ROC-AUC=0.816  PR-AUC=0.792  Brier=0.177\n",
      "[Fold 4] ROC-AUC=0.831  PR-AUC=0.839  Brier=0.172\n",
      "[Fold 5] ROC-AUC=0.810  PR-AUC=0.795  Brier=0.179\n",
      "\n",
      "OOF:\n",
      "ROC-AUC=0.822  PR-AUC=0.809  Brier=0.175\n",
      "Per-fold means ± SD:\n",
      "AUC  : 0.822 ± 0.008\n",
      "PR   : 0.810 ± 0.017\n",
      "Brier: 0.175 ± 0.003\n",
      "\n",
      "=== hist_gb ===\n",
      "[Fold 1] ROC-AUC=0.859  PR-AUC=0.835  Brier=0.152\n",
      "[Fold 2] ROC-AUC=0.847  PR-AUC=0.835  Brier=0.159\n",
      "[Fold 3] ROC-AUC=0.843  PR-AUC=0.813  Brier=0.159\n",
      "[Fold 4] ROC-AUC=0.869  PR-AUC=0.875  Brier=0.148\n",
      "[Fold 5] ROC-AUC=0.837  PR-AUC=0.825  Brier=0.165\n",
      "\n",
      "OOF:\n",
      "ROC-AUC=0.851  PR-AUC=0.835  Brier=0.157\n",
      "Per-fold means ± SD:\n",
      "AUC  : 0.851 ± 0.012\n",
      "PR   : 0.836 ± 0.021\n",
      "Brier: 0.157 ± 0.006\n",
      "\n",
      "Saved → artifacts/oof_baselines.parquet\n",
      "Saved → artifacts/baseline_summary.csv\n",
      "\n",
      "Baseline OOF summary (sorted by AUC):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>auc_oof</th>\n",
       "      <th>prauc_oof</th>\n",
       "      <th>brier_oof</th>\n",
       "      <th>auc_mean</th>\n",
       "      <th>auc_sd</th>\n",
       "      <th>pr_mean</th>\n",
       "      <th>pr_sd</th>\n",
       "      <th>brier_mean</th>\n",
       "      <th>brier_sd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hist_gb</td>\n",
       "      <td>0.851273</td>\n",
       "      <td>0.835078</td>\n",
       "      <td>0.156635</td>\n",
       "      <td>0.851026</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>0.836309</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.156635</td>\n",
       "      <td>0.005911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>0.821932</td>\n",
       "      <td>0.809246</td>\n",
       "      <td>0.175141</td>\n",
       "      <td>0.822027</td>\n",
       "      <td>0.008290</td>\n",
       "      <td>0.810209</td>\n",
       "      <td>0.016750</td>\n",
       "      <td>0.175140</td>\n",
       "      <td>0.002855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>lr_elasticnet</td>\n",
       "      <td>0.809280</td>\n",
       "      <td>0.792245</td>\n",
       "      <td>0.178775</td>\n",
       "      <td>0.810504</td>\n",
       "      <td>0.009604</td>\n",
       "      <td>0.795111</td>\n",
       "      <td>0.015837</td>\n",
       "      <td>0.178775</td>\n",
       "      <td>0.004088</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model   auc_oof  prauc_oof  brier_oof  auc_mean    auc_sd  \\\n",
       "2        hist_gb  0.851273   0.835078   0.156635  0.851026  0.011523   \n",
       "1  random_forest  0.821932   0.809246   0.175141  0.822027  0.008290   \n",
       "0  lr_elasticnet  0.809280   0.792245   0.178775  0.810504  0.009604   \n",
       "\n",
       "    pr_mean     pr_sd  brier_mean  brier_sd  \n",
       "2  0.836309  0.020761    0.156635  0.005911  \n",
       "1  0.810209  0.016750    0.175140  0.002855  \n",
       "0  0.795111  0.015837    0.178775  0.004088  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Baseline Suite: LR-ElasticNet, RandomForest, HistGradientBoosting\n",
    "# (StratifiedGroupKFold, group = STUDY_ID; no domain grouping)\n",
    "\n",
    "import json, joblib, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa: F401\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ---------- Load artifacts/spec ----------\n",
    "preprocessor_sparse = joblib.load(\"artifacts/preprocessor_unfitted.joblib\")\n",
    "with open(\"artifacts/preprocessor_spec.json\", \"r\") as f:\n",
    "    spec = json.load(f)\n",
    "\n",
    "TARGET   = spec[\"target\"]\n",
    "EXCLUDED = spec[\"excluded\"]\n",
    "\n",
    "# ---------- Build X/y and groups ----------\n",
    "drop_cols = list(set(EXCLUDED + [TARGET]))\n",
    "X_all = df_mod.drop(columns=drop_cols).copy()\n",
    "y_all = df_mod[TARGET].astype(int).copy()\n",
    "groups = df_mod[\"study_id\"].values\n",
    "\n",
    "# ---------- Helper: dense preprocessor for HGB ----------\n",
    "present_cont = spec[\"present_cont\"]\n",
    "present_cat  = spec[\"present_cat\"]\n",
    "present_bin  = spec[\"present_bin\"]\n",
    "\n",
    "def make_dense_preprocessor():\n",
    "    try:\n",
    "        ohe_dense = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "    except TypeError:\n",
    "        ohe_dense = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "\n",
    "    cont_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale\",  RobustScaler()),\n",
    "    ])\n",
    "    cat_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"ohe\",    ohe_dense),\n",
    "    ])\n",
    "    bin_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    ])\n",
    "\n",
    "    return ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"cont\", cont_pipe, present_cont),\n",
    "            (\"cat\",  cat_pipe,  present_cat),\n",
    "            (\"bin\",  bin_pipe,  present_bin),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False\n",
    "    )\n",
    "\n",
    "preprocessor_dense = make_dense_preprocessor()\n",
    "\n",
    "# ---------- Models ----------\n",
    "models = {\n",
    "    \"lr_elasticnet\": Pipeline([\n",
    "        (\"prep\", preprocessor_sparse),\n",
    "        (\"clf\", LogisticRegression(\n",
    "            penalty=\"elasticnet\",\n",
    "            l1_ratio=0.2,        # light L1 for stability; can tune 0.1–0.5\n",
    "            solver=\"saga\",       # supports elastic net + sparse\n",
    "            C=1.0,\n",
    "            max_iter=5000,\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"random_forest\": Pipeline([\n",
    "        (\"prep\", preprocessor_sparse),\n",
    "        (\"clf\", RandomForestClassifier(\n",
    "            n_estimators=500,\n",
    "            max_depth=None,\n",
    "            min_samples_split=4,\n",
    "            min_samples_leaf=2,\n",
    "            max_features=\"sqrt\",\n",
    "            n_jobs=-1,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "    \"hist_gb\": Pipeline([\n",
    "        (\"prep\", preprocessor_dense),  # dense\n",
    "        (\"clf\", HistGradientBoostingClassifier(\n",
    "            loss=\"log_loss\",\n",
    "            learning_rate=0.08,\n",
    "            max_depth=None,\n",
    "            max_bins=255,\n",
    "            l2_regularization=0.0,\n",
    "            early_stopping=True,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# ---------- CV + OOF evaluation ----------\n",
    "Path(\"artifacts\").mkdir(exist_ok=True)\n",
    "n_splits = 5\n",
    "cv = StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "all_oof = {}\n",
    "summary_rows = []\n",
    "\n",
    "for name, pipe in models.items():\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    oof_pred = np.zeros(len(y_all), dtype=float)\n",
    "    fold_metrics = []\n",
    "\n",
    "    for fold, (tr_idx, va_idx) in enumerate(cv.split(X_all, y_all, groups=groups), start=1):\n",
    "        X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "        y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "\n",
    "        model = pipe\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        p = model.predict_proba(X_va)[:, 1]\n",
    "        oof_pred[va_idx] = p\n",
    "\n",
    "        auc  = roc_auc_score(y_va, p)\n",
    "        pr   = average_precision_score(y_va, p)\n",
    "        bri  = brier_score_loss(y_va, p)\n",
    "        fold_metrics.append((auc, pr, bri))\n",
    "        print(f\"[Fold {fold}] ROC-AUC={auc:.3f}  PR-AUC={pr:.3f}  Brier={bri:.3f}\")\n",
    "\n",
    "    # Overall OOF\n",
    "    auc_oof = roc_auc_score(y_all, oof_pred)\n",
    "    pr_oof  = average_precision_score(y_all, oof_pred)\n",
    "    bri_oof = brier_score_loss(y_all, oof_pred)\n",
    "\n",
    "    fm = np.array(fold_metrics)\n",
    "    print(\"\\nOOF:\")\n",
    "    print(f\"ROC-AUC={auc_oof:.3f}  PR-AUC={pr_oof:.3f}  Brier={bri_oof:.3f}\")\n",
    "    print(\"Per-fold means ± SD:\")\n",
    "    print(f\"AUC  : {fm[:,0].mean():.3f} ± {fm[:,0].std():.3f}\")\n",
    "    print(f\"PR   : {fm[:,1].mean():.3f} ± {fm[:,1].std():.3f}\")\n",
    "    print(f\"Brier: {fm[:,2].mean():.3f} ± {fm[:,2].std():.3f}\")\n",
    "\n",
    "    all_oof[name] = oof_pred\n",
    "    summary_rows.append({\n",
    "        \"model\": name,\n",
    "        \"auc_oof\": auc_oof,\n",
    "        \"prauc_oof\": pr_oof,\n",
    "        \"brier_oof\": bri_oof,\n",
    "        \"auc_mean\": fm[:,0].mean(), \"auc_sd\": fm[:,0].std(),\n",
    "        \"pr_mean\": fm[:,1].mean(),  \"pr_sd\": fm[:,1].std(),\n",
    "        \"brier_mean\": fm[:,2].mean(), \"brier_sd\": fm[:,2].std(),\n",
    "    })\n",
    "\n",
    "# ---------- Save artifacts ----------\n",
    "oof_df = pd.DataFrame(all_oof, index=df_mod.index)\n",
    "oof_df.to_parquet(\"artifacts/oof_baselines.parquet\", index=True)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values(\"auc_oof\", ascending=False)\n",
    "summary_df.to_csv(\"artifacts/baseline_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved → artifacts/oof_baselines.parquet\")\n",
    "print(\"Saved → artifacts/baseline_summary.csv\")\n",
    "print(\"\\nBaseline OOF summary (sorted by AUC):\")\n",
    "display(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1fd9c-1983-4f38-9407-4d0910e5c33b",
   "metadata": {},
   "source": [
    "## Step C1: XGBoost with the same preprocessor and patient-level grouped CV, using light, nested tuning (no leakage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdbca896-ef3d-4d42-abdd-fbcb13c49f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Outer Fold 1] ROC-AUC=0.838  PR-AUC=0.815  Brier=0.165\n",
      "  Best params: {'clf__subsample': np.float64(0.7), 'clf__reg_lambda': 1.0, 'clf__reg_alpha': 0.0, 'clf__min_child_weight': 1.0, 'clf__max_depth': 4, 'clf__max_delta_step': 2, 'clf__learning_rate': np.float64(0.01), 'clf__gamma': 0.5, 'clf__colsample_bytree': np.float64(0.6)}\n",
      "[Outer Fold 2] ROC-AUC=0.829  PR-AUC=0.823  Brier=0.168\n",
      "  Best params: {'clf__subsample': np.float64(0.65), 'clf__reg_lambda': 0.0, 'clf__reg_alpha': 0.5, 'clf__min_child_weight': 1.0, 'clf__max_depth': 6, 'clf__max_delta_step': 0, 'clf__learning_rate': np.float64(0.01), 'clf__gamma': 0.1, 'clf__colsample_bytree': np.float64(0.85)}\n",
      "[Outer Fold 3] ROC-AUC=0.820  PR-AUC=0.792  Brier=0.172\n",
      "  Best params: {'clf__subsample': np.float64(0.65), 'clf__reg_lambda': 1.0, 'clf__reg_alpha': 0.0, 'clf__min_child_weight': 1.0, 'clf__max_depth': 5, 'clf__max_delta_step': 2, 'clf__learning_rate': np.float64(0.01), 'clf__gamma': 0.5, 'clf__colsample_bytree': np.float64(0.7)}\n",
      "[Outer Fold 4] ROC-AUC=0.834  PR-AUC=0.842  Brier=0.166\n",
      "  Best params: {'clf__subsample': np.float64(0.75), 'clf__reg_lambda': 2.0, 'clf__reg_alpha': 0.1, 'clf__min_child_weight': 1.0, 'clf__max_depth': 5, 'clf__max_delta_step': 2, 'clf__learning_rate': np.float64(0.019999999999999997), 'clf__gamma': 1.0, 'clf__colsample_bytree': np.float64(0.8999999999999999)}\n",
      "[Outer Fold 5] ROC-AUC=0.813  PR-AUC=0.803  Brier=0.175\n",
      "  Best params: {'clf__subsample': np.float64(0.6), 'clf__reg_lambda': 0.5, 'clf__reg_alpha': 0.1, 'clf__min_child_weight': 1.0, 'clf__max_depth': 4, 'clf__max_delta_step': 0, 'clf__learning_rate': np.float64(0.01), 'clf__gamma': 1.0, 'clf__colsample_bytree': np.float64(0.65)}\n",
      "\n",
      "=== Cross-validated (OOF) performance — XGBoost (light tuned) ===\n",
      "ROC-AUC=0.827  PR-AUC=0.815  Brier=0.169\n",
      "\n",
      "Per-fold means ± SD:\n",
      "AUC  : 0.827 ± 0.009\n",
      "PR   : 0.815 ± 0.017\n",
      "Brier: 0.169 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "# C1) XGBoost with patient-grouped nested CV (light tuning) → OOF metrics\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 0) Load artifacts from Step A\n",
    "preprocessor = joblib.load(\"artifacts/preprocessor_unfitted.joblib\")\n",
    "with open(\"artifacts/preprocessor_spec.json\", \"r\") as f:\n",
    "    spec = json.load(f)\n",
    "\n",
    "TARGET   = spec[\"target\"]\n",
    "EXCLUDED = spec[\"excluded\"]\n",
    "\n",
    "# 1) Build X/y; set patient groups (STUDY_ID)\n",
    "drop_cols = list(set(EXCLUDED + [TARGET]))\n",
    "X_all = df_mod.drop(columns=drop_cols).copy()\n",
    "y_all = df_mod[TARGET].astype(int).copy()\n",
    "\n",
    "if \"study_id\" not in df_mod.columns:\n",
    "    raise ValueError(\"study_id column is required for patient-level grouped CV.\")\n",
    "groups_all = df_mod[\"study_id\"].values\n",
    "\n",
    "# 2) Base pipeline (preprocessor + XGB); parameters to be tuned lightly\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=800,       # use enough trees; learning_rate tuned below\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\",  xgb),\n",
    "])\n",
    "\n",
    "# 3) Light search space (compact but effective)\n",
    "param_dist = {\n",
    "    \"clf__max_depth\":        [3, 4, 5, 6],\n",
    "    \"clf__learning_rate\":    np.linspace(0.01, 0.06, 6),\n",
    "    \"clf__subsample\":        np.linspace(0.6, 0.95, 8),\n",
    "    \"clf__colsample_bytree\": np.linspace(0.6, 0.95, 8),\n",
    "    \"clf__min_child_weight\": [1.0, 3.0, 5.0, 7.0],\n",
    "    \"clf__reg_lambda\":       [0.0, 0.5, 1.0, 2.0, 5.0],\n",
    "    \"clf__reg_alpha\":        [0.0, 0.1, 0.5, 1.0],\n",
    "    \"clf__gamma\":            [0.0, 0.1, 0.5, 1.0],\n",
    "    \"clf__max_delta_step\":   [0, 1, 2],\n",
    "}\n",
    "\n",
    "# 4) Outer CV (patient-grouped); inner CV for tuning on train folds only\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "oof_pred = np.zeros(len(y_all), dtype=float)\n",
    "fold_metrics = []\n",
    "fold_params  = []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(outer_cv.split(X_all, y_all, groups=groups_all), start=1):\n",
    "    X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "    groups_tr  = groups_all[tr_idx]\n",
    "\n",
    "    # Inner CV for tuning (grouped by patient)\n",
    "    inner_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=43 + fold)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=40,                 # light tuning\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=inner_cv.split(X_tr, y_tr, groups=groups_tr),\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=123 + fold,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    search.fit(X_tr, y_tr)\n",
    "    best_pipe = search.best_estimator_\n",
    "    fold_params.append(search.best_params_)\n",
    "\n",
    "    # Predict on the held-out outer fold\n",
    "    p = best_pipe.predict_proba(X_va)[:, 1]\n",
    "    oof_pred[va_idx] = p\n",
    "\n",
    "    auc  = roc_auc_score(y_va, p)\n",
    "    pr   = average_precision_score(y_va, p)\n",
    "    bri  = brier_score_loss(y_va, p)\n",
    "    fold_metrics.append((auc, pr, bri))\n",
    "    print(f\"[Outer Fold {fold}] ROC-AUC={auc:.3f}  PR-AUC={pr:.3f}  Brier={bri:.3f}\")\n",
    "    print(f\"  Best params: {search.best_params_}\")\n",
    "\n",
    "# 5) Overall OOF performance\n",
    "auc_oof = roc_auc_score(y_all, oof_pred)\n",
    "pr_oof  = average_precision_score(y_all, oof_pred)\n",
    "bri_oof = brier_score_loss(y_all, oof_pred)\n",
    "\n",
    "print(\"\\n=== Cross-validated (OOF) performance — XGBoost (light tuned) ===\")\n",
    "print(f\"ROC-AUC={auc_oof:.3f}  PR-AUC={pr_oof:.3f}  Brier={bri_oof:.3f}\")\n",
    "\n",
    "# 6) Per-fold summary\n",
    "fold_metrics = np.array(fold_metrics)\n",
    "print(\"\\nPer-fold means ± SD:\")\n",
    "print(f\"AUC  : {fold_metrics[:,0].mean():.3f} ± {fold_metrics[:,0].std():.3f}\")\n",
    "print(f\"PR   : {fold_metrics[:,1].mean():.3f} ± {fold_metrics[:,1].std():.3f}\")\n",
    "print(f\"Brier: {fold_metrics[:,2].mean():.3f} ± {fold_metrics[:,2].std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310217a-b1a4-4a73-b1be-f292b0b12149",
   "metadata": {},
   "source": [
    "# Step C2 - XGBoost Hyperparameter Tuning + Isotonic Calibration with Patient-Grouped Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fbe1684f-235c-4712-bd6a-9e3f05321a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-shikshuk/.local/lib/python3.9/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 1] ROC-AUC=0.844  PR-AUC=0.812  Brier=0.160\n",
      "  Best params: {'clf__subsample': np.float64(0.8), 'clf__reg_lambda': 5.0, 'clf__reg_alpha': 0.1, 'clf__min_child_weight': 3, 'clf__max_depth': 6, 'clf__learning_rate': np.float64(0.005), 'clf__gamma': 0.3, 'clf__colsample_bytree': np.float64(0.65)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-shikshuk/.local/lib/python3.9/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 2] ROC-AUC=0.836  PR-AUC=0.810  Brier=0.163\n",
      "  Best params: {'clf__subsample': np.float64(0.65), 'clf__reg_lambda': 2.0, 'clf__reg_alpha': 0.5, 'clf__min_child_weight': 3, 'clf__max_depth': 5, 'clf__learning_rate': np.float64(0.014000000000000002), 'clf__gamma': 0.1, 'clf__colsample_bytree': np.float64(0.65)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-shikshuk/.local/lib/python3.9/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 3] ROC-AUC=0.823  PR-AUC=0.786  Brier=0.168\n",
      "  Best params: {'clf__subsample': np.float64(0.6), 'clf__reg_lambda': 0.0, 'clf__reg_alpha': 0.5, 'clf__min_child_weight': 1, 'clf__max_depth': 6, 'clf__learning_rate': np.float64(0.005), 'clf__gamma': 0.1, 'clf__colsample_bytree': np.float64(0.85)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-shikshuk/.local/lib/python3.9/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Fold 4] ROC-AUC=0.843  PR-AUC=0.834  Brier=0.159\n",
      "  Best params: {'clf__subsample': np.float64(0.8), 'clf__reg_lambda': 5.0, 'clf__reg_alpha': 0.0, 'clf__min_child_weight': 3, 'clf__max_depth': 5, 'clf__learning_rate': np.float64(0.014000000000000002), 'clf__gamma': 0.0, 'clf__colsample_bytree': np.float64(0.6)}\n",
      "[Fold 5] ROC-AUC=0.817  PR-AUC=0.791  Brier=0.173\n",
      "  Best params: {'clf__subsample': np.float64(0.75), 'clf__reg_lambda': 5.0, 'clf__reg_alpha': 0.0, 'clf__min_child_weight': 1, 'clf__max_depth': 3, 'clf__learning_rate': np.float64(0.014000000000000002), 'clf__gamma': 0.1, 'clf__colsample_bytree': np.float64(0.85)}\n",
      "\n",
      "=== Cross-validated (OOF) — XGBoost expanded + calibrated ===\n",
      "ROC-AUC=0.836  PR-AUC=0.826  Brier=0.164\n",
      "\n",
      "Per-fold means ± SD:\n",
      "AUC  : 0.832 ± 0.011\n",
      "PR   : 0.807 ± 0.017\n",
      "Brier: 0.164 ± 0.005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter-shikshuk/.local/lib/python3.9/site-packages/sklearn/calibration.py:333: UserWarning: The `cv='prefit'` option is deprecated in 1.6 and will be removed in 1.8. You can use CalibratedClassifierCV(FrozenEstimator(estimator)) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# C2) XGBoost (expanded tuning + isotonic calibration) with patient-grouped CV\n",
    "import json, numpy as np, pandas as pd, joblib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedGroupKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 0) Load artifacts\n",
    "preprocessor = joblib.load(\"artifacts/preprocessor_unfitted.joblib\")\n",
    "with open(\"artifacts/preprocessor_spec.json\", \"r\") as f:\n",
    "    spec = json.load(f)\n",
    "\n",
    "TARGET, EXCLUDED = spec[\"target\"], spec[\"excluded\"]\n",
    "drop_cols = list(set(EXCLUDED + [TARGET]))\n",
    "X_all = df_mod.drop(columns=drop_cols).copy()\n",
    "y_all = df_mod[TARGET].astype(int).copy()\n",
    "groups_all = df_mod[\"study_id\"].values\n",
    "\n",
    "# 1) Base model\n",
    "xgb = XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    tree_method=\"hist\",\n",
    "    n_estimators=1200,\n",
    "    random_state=42\n",
    ")\n",
    "pipe = Pipeline([\n",
    "    (\"prep\", preprocessor),\n",
    "    (\"clf\",  xgb),\n",
    "])\n",
    "\n",
    "# 2) Expanded search space\n",
    "param_dist = {\n",
    "    \"clf__max_depth\":        [3, 4, 5, 6, 7],\n",
    "    \"clf__learning_rate\":    np.linspace(0.005, 0.05, 6),\n",
    "    \"clf__subsample\":        np.linspace(0.6, 1.0, 9),\n",
    "    \"clf__colsample_bytree\": np.linspace(0.6, 1.0, 9),\n",
    "    \"clf__min_child_weight\": [1, 3, 5, 7],\n",
    "    \"clf__gamma\":            [0.0, 0.1, 0.3, 0.5],\n",
    "    \"clf__reg_lambda\":       [0.0, 0.5, 1.0, 2.0, 5.0],\n",
    "    \"clf__reg_alpha\":        [0.0, 0.1, 0.5, 1.0],\n",
    "}\n",
    "\n",
    "# 3) Outer CV\n",
    "outer_cv = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "oof_pred, fold_metrics = np.zeros(len(y_all)), []\n",
    "\n",
    "for fold, (tr_idx, va_idx) in enumerate(outer_cv.split(X_all, y_all, groups=groups_all), start=1):\n",
    "    X_tr, X_va = X_all.iloc[tr_idx], X_all.iloc[va_idx]\n",
    "    y_tr, y_va = y_all.iloc[tr_idx], y_all.iloc[va_idx]\n",
    "    groups_tr  = groups_all[tr_idx]\n",
    "\n",
    "    # Inner tuning\n",
    "    inner_cv = StratifiedGroupKFold(n_splits=4, shuffle=True, random_state=43 + fold)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=60,\n",
    "        scoring=\"roc_auc\",\n",
    "        cv=inner_cv.split(X_tr, y_tr, groups=groups_tr),\n",
    "        n_jobs=-1,\n",
    "        refit=True,\n",
    "        random_state=123 + fold,\n",
    "        verbose=0\n",
    "    )\n",
    "    search.fit(X_tr, y_tr)\n",
    "    best_pipe = search.best_estimator_\n",
    "\n",
    "    # --- Calibration on validation fold ---\n",
    "    cal = CalibratedClassifierCV(best_pipe.named_steps[\"clf\"], cv=\"prefit\", method=\"isotonic\")\n",
    "    cal.fit(best_pipe.named_steps[\"prep\"].transform(X_va), y_va)\n",
    "\n",
    "    # Predict\n",
    "    p = cal.predict_proba(best_pipe.named_steps[\"prep\"].transform(X_va))[:, 1]\n",
    "    oof_pred[va_idx] = p\n",
    "\n",
    "    auc, pr, bri = roc_auc_score(y_va, p), average_precision_score(y_va, p), brier_score_loss(y_va, p)\n",
    "    fold_metrics.append((auc, pr, bri))\n",
    "    print(f\"[Fold {fold}] ROC-AUC={auc:.3f}  PR-AUC={pr:.3f}  Brier={bri:.3f}\")\n",
    "    print(f\"  Best params: {search.best_params_}\")\n",
    "\n",
    "# 4) Overall metrics\n",
    "auc_oof = roc_auc_score(y_all, oof_pred)\n",
    "pr_oof  = average_precision_score(y_all, oof_pred)\n",
    "bri_oof = brier_score_loss(y_all, oof_pred)\n",
    "\n",
    "print(\"\\n=== Cross-validated (OOF) — XGBoost expanded + calibrated ===\")\n",
    "print(f\"ROC-AUC={auc_oof:.3f}  PR-AUC={pr_oof:.3f}  Brier={bri_oof:.3f}\")\n",
    "\n",
    "fold_metrics = np.array(fold_metrics)\n",
    "print(\"\\nPer-fold means ± SD:\")\n",
    "print(f\"AUC  : {fold_metrics[:,0].mean():.3f} ± {fold_metrics[:,0].std():.3f}\")\n",
    "print(f\"PR   : {fold_metrics[:,1].mean():.3f} ± {fold_metrics[:,1].std():.3f}\")\n",
    "print(f\"Brier: {fold_metrics[:,2].mean():.3f} ± {fold_metrics[:,2].std():.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812217b0-db7a-435f-b269-541ee27a1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
